{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XanimGuliyeva/Spam_cClassification/blob/main/Spam_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BluuPoX5RpkJ"
      },
      "outputs": [],
      "source": [
        "# Install and Download Dataset\n",
        "!pip install kaggle transformers --quiet\n",
        "!kaggle datasets download -d wanderfj/enron-spam\n",
        "!unzip -q enron-spam.zip -d enron_email_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_lRytuBWjTf"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aGO1ZISrIj-"
      },
      "outputs": [],
      "source": [
        "# Download NLTK Resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Constants\n",
        "BASE_DIR = 'enron_email_dataset'\n",
        "ENRON_FOLDERS = [f'enron{i}' for i in range(1, 5)]\n",
        "HAM_DIR = os.path.join(BASE_DIR, 'ham')\n",
        "SPAM_DIR = os.path.join(BASE_DIR, 'spam')\n",
        "STOP_WORDS = set(stopwords.words('english')) | {'email', 'subject', 'hi'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_h_G4JjXzRw"
      },
      "outputs": [],
      "source": [
        "# Load Emails\n",
        "def load_all_emails(base_dir, folders):\n",
        "    messages, labels = [], []\n",
        "    for folder in folders:\n",
        "        ham_dir = os.path.join(base_dir, folder, 'ham')\n",
        "        spam_dir = os.path.join(base_dir, folder, 'spam')\n",
        "\n",
        "        # Load ham emails\n",
        "        for file in os.listdir(ham_dir):\n",
        "            with open(os.path.join(ham_dir, file), 'r', encoding='latin-1') as f:\n",
        "                messages.append(f.read())\n",
        "                labels.append('ham')\n",
        "\n",
        "        # Load spam emails\n",
        "        for file in os.listdir(spam_dir):\n",
        "            with open(os.path.join(spam_dir, file), 'r', encoding='latin-1') as f:\n",
        "                messages.append(f.read())\n",
        "                labels.append('spam')\n",
        "\n",
        "    return messages, labels\n",
        "\n",
        "all_messages, all_labels = load_all_emails(BASE_DIR, ENRON_FOLDERS)\n",
        "\n",
        "# Create a DataFrame\n",
        "emails = pd.DataFrame({\n",
        "    'Label': all_labels,\n",
        "    'Message': all_messages\n",
        "})\n",
        "\n",
        "# Display dataset summary\n",
        "print(emails.head())\n",
        "print(f\"Dataset contains {len(emails)} emails.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "8lnTSAuw2o0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Counts of Spam and Ham Emails\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=emails, x='Label', palette=\"viridis\")\n",
        "plt.title(\"Distribution of Ham and Spam Emails\")\n",
        "plt.xlabel(\"Email Type (0: Ham, 1: Spam)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(ticks=[0, 1], labels=['Ham', 'Spam'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D97YMkO72kCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPZbwjheYiHH"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text.lower())\n",
        "    return ' '.join([word for word in text.split() if word not in STOP_WORDS])\n",
        "\n",
        "emails['Cleaned_Message'] = emails['Message'].apply(preprocess_text)\n",
        "emails['Label'] = emails['Label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Print preprocessed data\n",
        "print(\"Sample of preprocessed emails:\")\n",
        "print(emails[['Message', 'Cleaned_Message', 'Label']].head())\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    emails['Cleaned_Message'], emails['Label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"\\nTraining data size: {len(X_train)}\")\n",
        "print(f\"Testing data size: {len(X_test)}\")\n",
        "\n",
        "# Print a sample of training data\n",
        "print(\"\\nSample training data:\")\n",
        "for i in range(5):\n",
        "    print(f\"Email: {X_train.iloc[i]}\")\n",
        "    print(f\"Label: {y_train.iloc[i]}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Most Frequent Words\n",
        "all_words = ' '.join(emails['Cleaned_Message'])\n",
        "word_freq = Counter(all_words.split())\n",
        "most_common_words = word_freq.most_common(20)"
      ],
      "metadata": {
        "id": "0e_chaPZ2qwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Most Frequent Words\n",
        "words, frequencies = zip(*most_common_words)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Words in All Emails')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQyED0-G2smM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Frequent Words in Emails')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "reSScdZn2uqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Most Frequent Words for Spam Emails\n",
        "spam_words = ' '.join(emails[emails['Label'] == 1]['Cleaned_Message'])\n",
        "spam_word_freq = Counter(spam_words.split())\n",
        "spam_most_common = spam_word_freq.most_common(20)\n",
        "\n",
        "# Plot Most Frequent Words in Spam Emails\n",
        "spam_words, spam_freqs = zip(*spam_most_common)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(spam_words, spam_freqs, color='red')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Words in Spam Emails')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ODzxzp432wgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Most Frequent Words for Ham Emails\n",
        "ham_words = ' '.join(emails[emails['Label'] == 0]['Cleaned_Message'])\n",
        "ham_word_freq = Counter(ham_words.split())\n",
        "ham_most_common = ham_word_freq.most_common(20)\n",
        "\n",
        "# Plot Most Frequent Words in Ham Emails\n",
        "ham_words, ham_freqs = zip(*ham_most_common)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(ham_words, ham_freqs, color='green')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Words in Ham Emails')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3OVHq8A32zrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Visualizations as Files\n",
        "wordcloud.to_file(\"wordcloud_all_emails.png\")"
      ],
      "metadata": {
        "id": "J4z4hc4L23zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in 'Label' column before mapping:\")\n",
        "print(emails['Label'].unique())\n"
      ],
      "metadata": {
        "id": "C-P_n9ZLJdOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Visualize Counts of Spam and Ham Emails\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=emails, x='Label', palette=\"viridis\")\n",
        "plt.title(\"Distribution of Ham and Spam Emails\")\n",
        "plt.xlabel(\"Email Type (0: Ham, 1: Spam)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(ticks=[0, 1], labels=['Ham', 'Spam'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ld0EXLth7ix_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSVLeTWltFVv"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the mapping of tokens to IDs\n",
        "token_to_id = vectorizer.vocabulary_\n",
        "\n",
        "# Print the token-to-ID mapping\n",
        "print(token_to_id)\n"
      ],
      "metadata": {
        "id": "J55aMYGOCyEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tokenized vocabulary\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "RHnn4Mp2CFVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Naive Bayes Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_nb))"
      ],
      "metadata": {
        "id": "nlWxzpKaZ0oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSjF-GnsxEwW"
      },
      "outputs": [],
      "source": [
        "# Plot Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_nb)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam'])\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_6OLfRwvRJe"
      },
      "outputs": [],
      "source": [
        "# Save Naive Bayes Model\n",
        "import joblib\n",
        "joblib.dump(nb_model, 'naive_bayes_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU Availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "g-93ZLcqbtKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilBertTokenizer.from_pretrained(...) - \tLoads a pre-trained tokenizer for tokenizing input text into a format suitable for DistilBERT.\n",
        "DistilBertForSequenceClassification.from_pretrained(...) - Loads a pre-trained DistilBERT model fine-tuned for classification tasks.\n"
      ],
      "metadata": {
        "id": "nHfVRp1einwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npKx_bzptMk-"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMbkCyss1Ctm"
      },
      "outputs": [],
      "source": [
        "class EmailDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.encodings = tokenizer(\n",
        "            list(texts),\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_len\n",
        "        )\n",
        "        # store labesl\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# Prepare Data for BERT\n",
        "train_dataset = EmailDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = EmailDataset(X_test.tolist(), y_test.tolist(), tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "cZ4r9ey-dr3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import EarlyStoppingCallback\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bTDzIgwJ9aTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = torch.tensor(predictions).argmax(dim=-1)\n",
        "    labels = torch.tensor(labels)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels.numpy(), predictions.numpy(), average=\"binary\")\n",
        "    acc = accuracy_score(labels.numpy(), predictions.numpy())\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "id": "EelJayD00B3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ppqVyIKnHGnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH1quMHj3AmE"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=True,\n",
        "    fp16=False,                                # Whether to use mixed precision training (False in this case)\n",
        "    #  means the model will use full precision\n",
        "    # It reduces memory usage and speeds up computations, especially on GPUs that support mixed precision\n",
        "    dataloader_num_workers=4,\n",
        "    metric_for_best_model=\"eval_loss\"\n",
        ")\n",
        "\n",
        "# Trainer Initialization\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,   # Include tokenizer for logging compatibility\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Available metrics: {eval_results.keys()}\")\n"
      ],
      "metadata": {
        "id": "yfFZ9T0E0NH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions and Evaluation\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred_bert = predictions.predictions.argmax(axis=-1)\n",
        "\n",
        "# Metrics\n",
        "print(f\"BERT Accuracy: {accuracy_score(y_test, y_pred_bert):.4f}\")\n",
        "print(\"\\nBERT Classification Report:\\n\", classification_report(y_test, y_pred_bert))"
      ],
      "metadata": {
        "id": "a6jt0Z6L_ZdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-4Fwate3Wi5"
      },
      "outputs": [],
      "source": [
        "# Group training loss by epoch\n",
        "training_logs = trainer.state.log_history\n",
        "train_loss_per_epoch = defaultdict(list)\n",
        "\n",
        "for log in training_logs:\n",
        "    if 'loss' in log and 'epoch' in log:\n",
        "        train_loss_per_epoch[int(log['epoch'])].append(log['loss'])\n",
        "\n",
        "# Compute average training loss per epoch\n",
        "avg_train_loss = [np.mean(train_loss_per_epoch[epoch]) for epoch in range(1, len(train_loss_per_epoch) + 1)]\n",
        "\n",
        "# Validation loss per epoch\n",
        "eval_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
        "\n",
        "# Ensure lengths match\n",
        "epochs = list(range(1, min(len(avg_train_loss), len(eval_loss)) + 1))\n",
        "avg_train_loss = avg_train_loss[:len(epochs)]\n",
        "eval_loss = eval_loss[:len(epochs)]\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, avg_train_loss, label='Training Loss (Epoch Average)', marker='o')\n",
        "plt.plot(epochs, eval_loss, label='Validation Loss', marker='o')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_true = predictions.label_ids                    # True labels\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_bert, labels=[0, 1])\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Ham\", \"Spam\"])\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for BERT Model\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a48i9TgRlD9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2unVkwphyPEj"
      },
      "outputs": [],
      "source": [
        "# Save BERT Model\n",
        "model.save_pretrained('./bert_saved_model')\n",
        "tokenizer.save_pretrained('./bert_saved_model')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}